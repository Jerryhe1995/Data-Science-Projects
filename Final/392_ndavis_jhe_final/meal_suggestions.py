# Noah Davis & Jie He
# CSC 392
# Final Project: Meal Suggestions
#
# This application suggests meals based on a user's nutrition log. It is an addition to my project for
# CSC 436, which was the SQL/Django back-end for a nutrition log API. We hope to work together over the
# summer to create a real world app using the skills we've learned in the data related classes that we
# took this semester!
#
# Programming for this project was split between Noah and Jie for the meal_suggestions
# meal logs were generated by Noah
#
## NOTE: TODO: statements in this file are a reminder for us, since we will be working on this project
##       TODO: over the summer and want to remember what needs to be changed. We don't have time to
##       TODO: implement these changes for now and they are not relevant to the final project requirements
##       TODO: for CSC 392.
##
## NOTE2: Users for this dataset range from 1 to 50. Users that ALWAYS gain weight may generate a comparison errror
##        Please test this file using user 1 and user 47. (Though ost of them work)
##        The reason for this error is the sample set. Since this is a proof of concept we are using a weird set
##        in which some users may always gain weight. In the real world tests we will ensure that at least one
##        (and probably many more) meals will result in a down trending weight, since we will base sampling
##        off of a much larger set of meals and feature heuristics for good and bad meals. (sorta TODO)

# Imports
import pandas, random
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from bootstrap import bootstrap

# TODO: modify to include a user_in argument then call from main file
#
def rec_meal():

    # Creates a neural network to make predictions based on the input nutritional logs
    # returns the model and the model's accuracy
    def neural_network(Xin, Yin):
        X = Xin
        Y = Yin
        #neural neural_network
        #print ("************ANN2**************")
        model = MLPClassifier(max_iter = 2000)

        #grid search
        param_grid = {'hidden_layer_sizes':[(5,), (6,), (7,), (8,), (9,), (10,),
                                            (11,), (12,), (13,), (14,), (15,), (16,),
                                            (17,), (18,), (19,), (20,)]}
        grid = GridSearchCV(model, param_grid, cv=5)
        grid.fit(X,Y)
        #print ("Grid Search: best parameters:{}".format(grid.best_params_))

        #evaluate the best model
        best_model = grid.best_estimator_

        predict_y = best_model.predict(X)
        #print("Accuracy:{}".format(accuracy_score(Y,predict_y)))

        model_stats = accuracy_score(Y,predict_y)

        #build the confusion matrix
        labels=[2,1,0]
        cm = confusion_matrix(Y,predict_y,labels=labels)
        cm_df = pandas.DataFrame(cm, index=labels, columns=labels)
        #print("Confusion Matrix:\n{}".format(cm_df))

        #bootstrapped confidence interval
        #print("Confidence interval best MLP:{}".format(bootstrap(best_model,working_df,'weight')))
        return best_model, model_stats

    # Creates a decision tree model based on a given nutritional logs
    # returns the model and the model's accuracy
    def tree(Xin, Yin):

        X = Xin
        Y = Yin

        #Tree
        #print ("************Tree**************")
        model = DecisionTreeClassifier()

        #Grid Search
        param_grid = {'max_depth':list(range(1,31)),'criterion':['entropy','gini']}
        grid = GridSearchCV(model, param_grid, cv=5)
        grid.fit(X,Y)
        #print ("Grid Search: best parameters:{}".format(grid.best_params_))

        #evaluate the best model
        best_model = grid.best_estimator_
        predict_y = best_model.predict(X)
        #print("Accuracy:{}".format(accuracy_score(Y,predict_y)))

        model_stats['tree'] = accuracy_score(Y,predict_y)

        #build confusion matrix
        labels=[2,1,0]
        cm = confusion_matrix(Y,predict_y,labels=labels)
        cm_df = pandas.DataFrame(cm, index=labels, columns=labels)
        #print("Confusion Matrix:\n{}".format(cm_df))

        #bootstrapped confidence interval
        #print("Confidence interval best MLP:{}".format(bootstrap(best_model,working_df,'weight')))
        return best_model, model_stats

    # Creates a KNN model based on given nutritional logs
    # returns the model and the model's accuracy
    def knn(Xin, Yin):

        X = Xin
        Y = Yin

        #KNN
        #print ("************KNN**************")
        model = KNeighborsClassifier()

        #grid search
        param_grid = {'n_neighbors':list(range(1,31))}
        grid = GridSearchCV(model, param_grid, cv=5)
        grid.fit(X,Y)
        #print ("Grid Search: best parameters:{}".format(grid.best_params_))

        #evaluate the best model
        best_model = grid.best_estimator_
        predict_y = best_model.predict(X)
        #print("Accuracy:{}".format(accuracy_score(Y,predict_y)))

        model_stats = accuracy_score(Y,predict_y)

        #build confusion matrix
        labels=[2,1,0]
        cm = confusion_matrix(Y,predict_y,labels=labels)
        cm_df = pandas.DataFrame(cm, index=labels, columns=labels)
        #print("Confusion Matrix:\n{}".format(cm_df))

        #bootstrapped confidence interval
        #print("Confidence interval best MLP:{}".format(bootstrap(best_model,working_df,'weight')))
        return best_model, model_stats


    ### BEGIN USER PROCESSING
    ##
    ## TODO: In a real analytics engine this routine will be placed in a different file
    ## TODO: in order to adhere to object-oriented design practices
    ## But for this project we don't care so much D:
    ##

    # in a true implementation this will be based upon web framework session variables
    user_id = input("Enter your user ID: ")

    # in a true implementation, a subset of meals would be queried from the database
    # ours is a database dump from our analytics_test MySQL database
    df = pandas.read_csv("meals.csv", sep=';')

    # Get the meals specific to a user to model based upon their individual features
    # No type checking because in a real case this will be generated from session variables

    per_person= df[ df['user_id'] == int(user_id) ]
    per_person.index=range(len(per_person['weight']))

    # Keep track of model accuracy (maybe some other stats later) in order to pick the best model
    model_stats = {}

    # Will display user name with session variables later
    print ("Welcome back, "+str(user_id)+"!")

    # meaningless for our implementation. In the future, will be used to keep
    # daily weight of a user for the next days log and new models. Intentionally
    # not used here.
    weight = input("What is your current weight (in lbs)?: ")


    # TODO: Real world needs better goals, same weight, amount of weight to lose, etc
    # TODO: which can be used to improve model and scoring of food. Not necessary for
    # TODO: this project.
    # 0 = Lose weight
    # 1 = Maintain weight
    # 2 = Gain weight
    goal = 0

    # Per person dataframe for individualized feature set
    new_df = per_person.drop(['id','log_date','user_id','mood'],axis=1)

    # enumerate data is a different way, for practice
    new_df['type'].replace(['breakfast', 'breakfast_drink', 'lunch', 'lunch_drink', 'dinner', 'dinner_drink', 'snack']
                           ,[1,2,3,4,5,6,7],inplace=True)

    change = []
    # print(new_df['weight'][7])
    for i in range(int((len(new_df['weight'])-1)/7)):
        # if weight is going up, indicate it is going up
        if int(new_df['weight'][i*7+7]) >= int(new_df['weight'][i*7]):
            for j in range(7):
                change.append(2)
        # if weight is staying same, indicate it is staying same
        elif int(new_df['weight'][i*7+7]) == int(new_df['weight'][i*7]):
            for j in range(7):
                change.append(1)
        # else, weight is going down
        else:
            for j in range(7):
              change.append(0)

    # Enumerate the foods, keep them in a dictionary for look-up
    food=[]
    food_dict= []

    for i in range( len( new_df['food'] )-1 ):
        if new_df['food'][i] not in food_dict:
            food.append(len(food_dict))
            food_dict.append(new_df['food'][i])
        else:
            for j in range(len(food_dict)):
                if new_df['food'][i]==food_dict[j]:
                    food.append(j)
    size = 7* int((len(new_df['weight'])-1)/7)

    # TODO: In the future I think it would be better to generate a logged meal for each serving
    # create a dataframe with our sample meals
    working_df = pandas.DataFrame({
        'protein': new_df['protein'][:size],
        'servings': new_df['servings'][:size],
        'cals': new_df['cals'][:size],
        'fat': new_df['fat'][:size],
        'carbs': new_df['carbs'][:size],
        'type': new_df['type'][:size],
        'food':food[:size],
        'weight':change
    })
    features  = working_df.drop(['weight'],axis=1)
    target  = working_df['weight']

    # Define the types of models that we can use
    nn = neural_network(features, target)[0]
    tree = tree(features, target)[0]
    knn = knn(features, target)[0]

    def get_best(models_in):
        for thing in models_in:
            best = thing
            if models_in[thing] > models_in[best]:
                best = thing
        return best

    best = get_best(model_stats)

    # Pick the best model from the models that we generated
    if best == 'knn':
        model = knn
    elif best == 'tree':
        model = tree
    elif best == 'nn':
        model = nn

    #model = nn

    # A set of foods from which to select
    # TODO: In a real implementation this set should be based on some heuristic values per-user
    # TODO: for now, we are using the same features we used to train, since we do not have real
    # TODO: values from a study or user testing.
    predict = features

    # This is currently the WHOLE set--in a true implementation
    # it will need to be a smaller subset based on some user
    # features as we explore how nutrition is influenced per-user
    predictions = model.predict(features)
    result_df = pandas.DataFrame(data={'weight': predictions})
    result = pandas.concat([predict, result_df], axis=1)
    result = result.drop_duplicates()

    best_foods = result[result.weight == goal]

    # Process the food into categories (breakfast, lunch, etc) and then recommend from the sets of food
    # plus some cleaning.
    foods = best_foods.drop(['cals','carbs','fat','protein','weight'], axis=1)
    # Get our types back as strings, not necessary but we are practicing!
    foods['type'].replace([1,2,3,4,5,6,7],['breakfast', 'breakfast_drink', 'lunch', 'lunch_drink', 'dinner', 'dinner_drink', 'snack']
                           ,inplace=True)
    food_recommendation= []
    for food in foods['food']:
        food_recommendation.append(food_dict[int(food)])

    food_to_eat = pandas.DataFrame({
        'food': food_recommendation,
        'servings': foods['servings'],
        'type':foods['type']
    })

    breakfast = food_to_eat[food_to_eat['type'] == 'breakfast']
    breakfast_drink = food_to_eat[food_to_eat['type'] == 'breakfast_drink']
    lunch = food_to_eat[food_to_eat['type'] == 'lunch']
    lunch_drink = food_to_eat[food_to_eat['type'] == 'lunch_drink']
    dinner = food_to_eat[food_to_eat['type'] == 'dinner']
    dinner_drink = food_to_eat[food_to_eat['type'] == 'dinner_drink']
    snack = food_to_eat[food_to_eat['type'] == 'snack']

    # We pick random meals from the food sets. We believe that with real-world data
    # the meals we pick can be narrowed down to a much smaller set by using better models
    # but for this proof-of-concept we aren't worried about it as much.
    #
    # We also hope to allow a user to list all of their recommended meals or get random
    # ones but that does not adhere to our project proposal so we're going to generate a
    # sort-of-random daily meal recommendation

    breakfast = breakfast.sample(n=1)
    breakfast = (breakfast.iloc[0]['servings'],breakfast.iloc[0]['food'])
    breakfast_drink = breakfast_drink.sample(n=1)
    breakfast_drink = (breakfast_drink.iloc[0]['servings'],breakfast_drink.iloc[0]['food'])
    lunch = lunch.sample(n=1)
    lunch = (lunch.iloc[0]['servings'],lunch.iloc[0]['food'])
    lunch_drink = lunch_drink.sample(n=1)
    lunch_drink= (lunch_drink.iloc[0]['servings'],lunch_drink.iloc[0]['food'])
    dinner = dinner.sample(n=1)
    dinner = (dinner.iloc[0]['servings'],dinner.iloc[0]['food'])
    dinner_drink = dinner_drink.sample(n=1)
    dinner_drink = (dinner_drink.iloc[0]['servings'],dinner_drink.iloc[0]['food'])
    snack = snack.sample(n=1)
    snack = (snack.iloc[0]['servings'],snack.iloc[0]['food'])


    print("\nYour meal plan for today has been generated,\n")

    print("Breakfast: "+str(breakfast[0])+" of "+str(breakfast[1])+" and "+str(breakfast_drink[0])+ " cup of " +str(breakfast_drink[1]))
    print("Lunch: "+str(lunch[0])+" of "+str(lunch[1])+" and "+str(lunch_drink[0])+ " cup of " +str(lunch_drink[1]))
    print("Dinner: "+str(dinner[0])+" of "+str(dinner[1])+" and "+str(dinner_drink[0])+ " cup of " +str(dinner_drink[1]))
    print("Snack: "+str(snack[0])+" of "+str(snack[1]))



# Main -- not separate for this since we are proving pandas/scikit knowledge and not OO design
rec_meal()


